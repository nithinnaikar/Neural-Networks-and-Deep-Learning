1. We use caching to pass variables computed during forward propagation to the corresponding backward propagation step. It contains useful values for backpropagation needed to 
compute derivatives (instead of recomputing the value again).
2. Hyperparameters include 1) Number of iterations 2) number of layers L in the neural network 3) size of the hidden layers n_l
3. The deeper layers of a neural network are typically computing more complex features of the input than the earlier layers.
4. False. A for loop is needed to compute activations over layers, not vectorization.
5. The 3rd code block allows you to initialize the parameters of the model.
6. The number of layers L is 4. The number of hidden layers is 3. 
7. True
8. True
9. 1) W1 will have shape (4, 4) 2) W2 will have shape (3, 4) 3) b1 will have shape (4, 1) 4) W3 will have shape (1, 3) 5) b3 will have shape (1, 1) 6) b2 will have shape (3, 1)
10. W_l has shape (n_1, n_l-1)

Grade Received: 100%
